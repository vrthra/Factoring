{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factoring Failure-Inducing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of the original delta debugging (and its descendents) is that if one has a failure inducing input, which contains multiple independent, but overlapping faults, each of which results in crashes that can not be differentiated, then during test input minimization, only one of these inputs will be found. We show how to extract all independent faults from a given input string.\n",
    "\n",
    "We start by importing the prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicate\n",
    "\n",
    "First, we define a way to capture the status of an input. There can be four outcomes when an input is executed:\n",
    "\n",
    "* Success (failure condition reproduced)\n",
    "* Failed (failure condition not reproduced)\n",
    "* Invalid (Did not reach failure condition -- possibly semantically invalid)\n",
    "* Timeout (equivalant to Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRes(str, Enum):\n",
    "    success = 'SUCCESS'\n",
    "    failed = 'FAILED'\n",
    "    invalid = 'INVALID'\n",
    "    timeout = 'TIMEOUT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our predicate. Our error condition is when either `< ... select ... >` or `< ... option ... >` is present in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_error(inp):\n",
    "    if re.match(r'.*<.*(select|option).*>.*', inp):\n",
    "        return PRes.success\n",
    "    else:\n",
    "        return PRes.failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\n",
    "'<select>',\n",
    "'<option>',\n",
    "'ab<opt>cd',\n",
    "'abc<option>def',\n",
    "'<<option>',\n",
    "'<opt>cd',\n",
    "'aa<option>xx<select>',\n",
    "'abc<option select>def'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRes.success\n",
      "PRes.success\n",
      "PRes.failed\n",
      "PRes.success\n",
      "PRes.success\n",
      "PRes.failed\n",
      "PRes.success\n",
      "PRes.success\n"
     ]
    }
   ],
   "source": [
    "for s in strings:\n",
    "    print(simple_error(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a partition length, we want to split the string into that many partitions, remove each partition one at a time from the string, and check if for any of them, the causal() succeeds. If it succeeds for any, then return the succeeding string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_check_each_fragment(instr, start, part_len, causal):\n",
    "    for i in range(start, len(instr), part_len):\n",
    "        stitched =  instr[:i] + instr[i+part_len:]\n",
    "        if causal(stitched): return i, stitched\n",
    "    return -1, instr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function. We start by the smallest number of partitions â€“ 2. Then, we check by removing each fragment for success. If removing one fragment succeeds, we change the current string to the string without that fragment. Since we succeeded in removing one fragment at this partition length, we do not know if we can remove other parts of the string. So, we want to redo with the same fragment length, so we keep the current partition length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If none of the fragments could be removed, then it is time to decrease the partition length. We reduce the partition length by half.\n",
    "\n",
    "If the partition length is now single chars, then we break and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddmin(cur_str, causal_fn):\n",
    "    start, part_len = 0, len(cur_str) // 2\n",
    "    while part_len >= 1:\n",
    "        start, cur_str = remove_check_each_fragment(cur_str, start, part_len, causal_fn)\n",
    "        if start != -1:\n",
    "            if not cur_str: return ''\n",
    "        else:\n",
    "            start, part_len = 0, part_len // 2\n",
    "    return cur_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_input(s):\n",
    "    return simple_error(s) == PRes.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_to_minimize = [s for s in strings if simple_error(s) == PRes.success]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: '<select>'\n",
      "<select>\n",
      "Input: '<option>'\n",
      "<option>\n",
      "Input: 'abc<option>def'\n",
      "<option>\n",
      "Input: '<<option>'\n",
      "<option>\n",
      "Input: 'aa<option>xx<select>'\n",
      "<select>\n",
      "Input: 'abc<option select>def'\n",
      "<select>\n"
     ]
    }
   ],
   "source": [
    "for s in strings_to_minimize:\n",
    "    print('Input:',repr(s))\n",
    "    solution = ddmin(s, test_input)\n",
    "    print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, running `ddmin` can result in identifying only a single fault when multiple faults are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple solution is to generate `N` input strings each string missing one delta each, and then run `ddmin` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_fii(inputstr):\n",
    "    results = set()\n",
    "    one_minus = [inputstr[:i] + inputstr[i+1:] for i in range(len(inputstr))]\n",
    "    for s in [inputstr] + one_minus:\n",
    "        if test_input(s):\n",
    "            solution = ddmin(s, test_input)\n",
    "            results.add(solution)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<option>', '<select>'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_fii('<option select>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: '<select>'\n",
      "{'<select>'}\n",
      "Input: '<option>'\n",
      "{'<option>'}\n",
      "Input: 'abc<option>def'\n",
      "{'<option>'}\n",
      "Input: '<<option>'\n",
      "{'<option>'}\n",
      "Input: 'aa<option>xx<select>'\n",
      "{'<select>', '<option>'}\n",
      "Input: 'abc<option select>def'\n",
      "{'<select>', '<option>'}\n"
     ]
    }
   ],
   "source": [
    "for s in strings_to_minimize:\n",
    "    print('Input:',repr(s))\n",
    "    solution = factor_fii(s)\n",
    "    print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our predicate. It is a simple test for doubled parenthesis such as `((..))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expr_double_paren(inp):\n",
    "    if re.match(r'.*[(][(].*[)][)].*', inp):\n",
    "        return PRes.success\n",
    "    return PRes.failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predicate = expr_double_paren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_input =  '1 + ((2 * 3 / 4))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = expr_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that we can reproduce the failing condition correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert my_predicate(my_input) == PRes.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert my_predicate('(1+2)') == PRes.failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A context-free grammar is represented as a Python dict, with each nonterminal symbol forming a key, and each nonterminal _defined_ by a list of expansion rules. For example, the expression grammar for parsing arithmetic expressions is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_GRAMMAR = {'<start>': [['<expr>']],\n",
    " '<expr>': [['<term>', ' + ', '<expr>'],\n",
    "  ['<term>', ' - ', '<expr>'],\n",
    "  ['<term>']],\n",
    " '<term>': [['<factor>', ' * ', '<term>'],\n",
    "  ['<factor>', ' / ', '<term>'],\n",
    "  ['<factor>']],\n",
    " '<factor>': [['+', '<factor>'],\n",
    "  ['-', '<factor>'],\n",
    "  ['(', '<expr>', ')'],\n",
    "  ['<integer>', '.', '<integer>'],\n",
    "  ['<integer>']],\n",
    " '<integer>': [['<digit>', '<integer>'], ['<digit>']],\n",
    " '<digit>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_G = {'[start]': '<start>', '[grammar]': EXPR_GRAMMAR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the convetion we used: Each nonterminal is enclosed in angle brackets. E.g. `<expr>`. We now define a function that can distinguish terminal symbols from nonterminals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `is_nt()` function checks if the given node is a terminal or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nt(symbol):\n",
    "     return symbol and (symbol[0], symbol[-1]) == ('<', '>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the grammar, and an input, we can parse it into a derivation tree.\n",
    "The `Parser` below is from [fuzzingbook.org](https://www.fuzzingbook.org/html/Parser.html), and provides a generic context-free parser. This is present in the `src` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import EarleyParser as Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we check that our parse succeeded? We can convert the derivation tree back to the original string and check for equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tree_to_string()` function converts a derivation tree to its original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_string(tree, wrap=lambda ntree, depth, name, string: string, depth=0):\n",
    "    name, children, *rest = tree\n",
    "    if not is_nt(name):\n",
    "        return name\n",
    "    else:\n",
    "        return wrap(tree, depth, name, ''.join([tree_to_string(c, wrap, depth-1) for c in children]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expr_parser = Parser(EXPR_GRAMMAR, start_symbol='<start>', canonical=True)\n",
    "parsed_expr = list(expr_parser.parse(my_input))[0]\n",
    "tree_to_string(parsed_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical trees\n",
    "\n",
    "While converting to strings are easy, it is unsatisfying. We want to make our output look pretty, and inspect the tree structure of the parsed tree. So we define graphical tree display (code from fuzzingbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(v, zoom=True):\n",
    "    # return v directly if you do not want to zoom out.\n",
    "    if zoom:\n",
    "        return Image(v.render(format='png'))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node(node, id):\n",
    "    symbol, children, *annotation = node\n",
    "    return symbol, children, ''.join(str(a) for a in annotation)\n",
    "def node_attr(dot, nid, symbol, ann): dot.node(repr(nid), symbol)\n",
    "def edge_attr(dot, start_node, stop_node): dot.edge(repr(start_node), repr(stop_node))\n",
    "def graph_attr(dot): dot.attr('node', shape='plain')\n",
    "def display_tree(derivation_tree):\n",
    "    counter = 0\n",
    "    def traverse_tree(dot, tree, id=0):\n",
    "        (symbol, children, annotation) = extract_node(tree, id)\n",
    "        node_attr(dot, id, symbol, annotation)\n",
    "        if children:\n",
    "            for child in children:\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                child_id = counter\n",
    "                edge_attr(dot, id, child_id)\n",
    "                traverse_tree(dot, child, child_id)\n",
    "    dot = Digraph(comment=\"Derivation Tree\")\n",
    "    graph_attr(dot)\n",
    "    traverse_tree(dot, derivation_tree)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to display the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zoom(display_tree(parsed_expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction\n",
    "\n",
    "We are now ready to define our delta debugging reduction phase. We use the **Perses** algorithm. The algorithm is as follows: We start the root, and recursively go down the child nodes. For each node, we check if that node can be replaced by a subtree with the same nonterminal, and still reproduce the failure, and find the smallest such tree (length determined by number of leaves).\n",
    "\n",
    "Since this procedure can result in multiple trees, the tree to work on is chosen based on a priority queue where the priority is given to the smallest tree.\n",
    "\n",
    "The particular node chosen to replace the current node is determined based first on its numer of leaf nodes, and then on its rank in a priority queue, where the priority is determined by the depth of the subtree from the current node. That is, a child gets priority over a grand child."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to define a way to address a specific node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = parsed_expr[1][0][1][2][1][0]\n",
    "zoom(display_tree(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree_to_string(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the path, we simply use a list of numbers indicating the child node. For example, in the above, the path would be `[0, 2, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path, `get_child()` will simply return the node at the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_child(tree, path):\n",
    "    if not path: return tree\n",
    "    cur, *path = path\n",
    "    return get_child(tree[1][cur], path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "te = get_child(parsed_expr, [0, 2, 0])\n",
    "zoom(display_tree(te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_string(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to replace one node with another. This is done by `replace_path()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_path(tree, path, new_node=None):\n",
    "    if new_node is None: new_node = []\n",
    "    if not path: return copy.deepcopy(new_node)\n",
    "    cur, *path = path\n",
    "    name, children, *rest = tree\n",
    "    new_children = []\n",
    "    for i,c in enumerate(children):\n",
    "        if i == cur:\n",
    "            nc = replace_path(c, path, new_node)\n",
    "        else:\n",
    "            nc = c\n",
    "        if nc:\n",
    "            new_children.append(nc)\n",
    "    return (name, new_children, *rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = parsed_expr[1][0][1][2][1][0]\n",
    "zoom(display_tree(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_string(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "te = replace_path(parsed_expr, [0, 2, 0], [])\n",
    "zoom(display_tree(te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_string(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = replace_path(parsed_expr, [0, 2, 0], ('x',[]))\n",
    "zoom(display_tree(tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_to_string(tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priority queue\n",
    "\n",
    "For perses reduction, one needs a way to count the number of leaf nodes to determine the priority of a node. This is done by `count_leaves()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_leaves(node):\n",
    "    name, children, *_ = node\n",
    "    if not children:\n",
    "        return 1\n",
    "    return sum(count_leaves(i) for i in children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_leaves(parsed_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_leaves(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a helper that simply counts the internal nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(node):\n",
    "    name, children, *_ = node\n",
    "    if not children:\n",
    "        return 0\n",
    "    return sum(count_nodes(i) for i in children) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_nodes(parsed_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to maintain a priority queue of the `[(tree, path)]`. The essential idea is to prioritize the items first by the number of leaves in  the full tree (that is, the smallest tree that we have currently gets priority), then next by the number of leaves in the node pointed to by path, and finally, tie break by the insertion order (`ecount`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_pq(tup, q):\n",
    "    global ecount\n",
    "    dtree, F_path = tup\n",
    "    stree = get_child(dtree, F_path)\n",
    "    n =  count_leaves(dtree)\n",
    "    m =  count_leaves(stree)\n",
    "    # heap smallest first\n",
    "    heapq.heappush(q, (n, m, -ecount, tup))\n",
    "    ecount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define another helper function `nt_group()` that groups all nonterminals that have the same name. These are used to determine the nodes that can be used to replace one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_group(tree, all_nodes=None):\n",
    "    if all_nodes is None: all_nodes = {}\n",
    "    name, children, *_ = tree\n",
    "    if not is_nt(name): return\n",
    "    all_nodes.setdefault(name, []).append(tree)\n",
    "    for c in children:\n",
    "        nt_group(c, all_nodes)\n",
    "    return all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = nt_group(te)\n",
    "for key in gp:\n",
    "    print(key)\n",
    "    for node in gp[key]:\n",
    "        print(tree_to_string(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the compatible nodes? These are all the nodes that have the same nonterminal name, and is a descendent of the current node. Further, if the nonterminal allows empty node, then this is the first in the list. This is defined by `compatible_nodes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compatible_nodes(tree, grammar):\n",
    "    key, children, *_ = tree\n",
    "    # Here is the first choice. Do we restrict ourselves to only children of the tree\n",
    "    # or do we allow all nodes in the original tree? given in all_nodes?\n",
    "    lst = nt_group(tree)\n",
    "    node_lst = [(i, n) for i,n in enumerate(lst[key])]\n",
    "\n",
    "    # insert empty if the grammar allows it as the first element\n",
    "    if [] in grammar[key]: node_lst.insert(0, (-1, (key, [])))\n",
    "    return node_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_child(te, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_nodes(get_child(te, [0]), EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some programming languages have `tokens` which are first level lexical elements. The parser is often defined using the lexer tokens. We do not want to try to reduce tokens further. So we define a way to identify them (we have to keep in mind when we produce grammars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token(val):\n",
    "    assert val != '<>'\n",
    "    assert (val[0], val[-1]) == ('<', '>')\n",
    "    if val[1].isupper(): return True\n",
    "    #if val[1] == '_': return val[2].isupper() # token derived.\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perses reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally define the reduction algorithm. The implementation of __Perses__ is given in `reduction()`. The essential idea is as follows:\n",
    "\n",
    "1. We have a priority queue of (tree, path_to_node) structures, where node is a node within the tree.\n",
    " * The highest priority is given to the smallest tree.\n",
    " * With in the nodes in the same tree, priority is given to nodes with smallest number of leaves\n",
    " * In case of tie break, the shallowest subnode gets the highest priority (i.e child has higher priority over grand child, and empty node has the highest priority since it is a peer of the current node).\n",
    "2. We pick each nodes, and find compatible subnodes that reproduce the failure.\n",
    "3. Each compatible node and the corresponding tree is put back into the priority queue.\n",
    "4. If no child nodes were found that could replace the current node, then we add each children with the current tree into the priority queue. (If we had to recurse into the child nodes, then the next tree that will get picked will be a different tree.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction(tree, grammar, predicate):\n",
    "    first_tuple = (tree, [])\n",
    "    p_q = []\n",
    "    add_to_pq(first_tuple, p_q)\n",
    "\n",
    "    ostr = tree_to_string(tree)\n",
    "    assert predicate(ostr) == PRes.success\n",
    "    failed_set = {ostr: True}\n",
    "\n",
    "    min_tree, min_tree_size = tree, count_leaves(tree)\n",
    "    while p_q:\n",
    "        # extract the tuple\n",
    "        _n, _m, _ec, (dtree, F_path) = heapq.heappop(p_q)\n",
    "        stree = get_child(dtree, F_path)\n",
    "        skey, schildren = stree\n",
    "        found = False\n",
    "        # we now want to replace stree with alternate nodes.\n",
    "        for i, node in compatible_nodes(stree, grammar):\n",
    "            # replace with current (copy).\n",
    "            ctree = replace_path(dtree, F_path, node)\n",
    "            if ctree is None: continue # same node\n",
    "            v = tree_to_string(ctree)\n",
    "            if v in failed_set: continue\n",
    "            failed_set[v] = predicate(v) # we ignore PRes.invalid results\n",
    "            if failed_set[v] == PRes.success:\n",
    "                found = True\n",
    "                ctree_size = count_leaves(ctree)\n",
    "                if ctree_size < min_tree_size: min_tree, min_tree_size = ctree, ctree_size\n",
    "\n",
    "                if v not in failed_set:\n",
    "                    print(v)\n",
    "                t = (ctree, F_path)\n",
    "                assert get_child(ctree, F_path) is not None\n",
    "                add_to_pq(t, p_q)\n",
    "\n",
    "        # The CHOICE here is that we explore the children if and only if we fail\n",
    "        # to find a node that can replace the current\n",
    "        if found: continue\n",
    "        if is_token(skey): continue # do not follow children TOKEN optimization\n",
    "        for i, child in enumerate(schildren):\n",
    "            if not is_nt(child[0]): continue\n",
    "            assert get_child(tree=dtree, path=F_path + [i]) is not None\n",
    "            t = (dtree, F_path + [i])\n",
    "            add_to_pq(t, p_q)\n",
    "    return min_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = reduction(parsed_expr, EXPR_GRAMMAR, my_predicate)\n",
    "zoom(display_tree(er))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_to_string(er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Fuzzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define abstraction, we need to be able to generate values based on a grammar. Our fuzzer is able to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fuzzer:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    def fuzz(self, key='<start>', max_num=None, max_depth=None):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fuzzer tries to randomly choose an expansion when more than one expansion is available. If however, it goes beyond max_depth, then it chooses the cheapest nodes. The cheapest nodes are those nodes with minimum further expansion (no recursion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(Fuzzer):\n",
    "    def symbol_cost(self, grammar, symbol, seen):\n",
    "        if symbol in self.key_cost: return self.key_cost[symbol]\n",
    "        if symbol in seen:\n",
    "            self.key_cost[symbol] = float('inf')\n",
    "            return float('inf')\n",
    "        v = min((self.expansion_cost(grammar, rule, seen | {symbol})\n",
    "                    for rule in grammar.get(symbol, [])), default=0)\n",
    "        self.key_cost[symbol] = v\n",
    "        return v\n",
    "\n",
    "    def expansion_cost(self, grammar, tokens, seen):\n",
    "        return max((self.symbol_cost(grammar, token, seen)\n",
    "                    for token in tokens if token in grammar), default=0) + 1\n",
    "\n",
    "    def gen_key(self, key, depth, max_depth):\n",
    "        if key not in self.grammar: return key\n",
    "        if depth > max_depth:\n",
    "            assert key in self.cost\n",
    "            clst = sorted([(self.cost[key][str(rule)], rule) for rule in self.grammar[key]])\n",
    "            rules = [r for c,r in clst if c == clst[0][0]]\n",
    "        else:\n",
    "            rules = self.grammar[key]\n",
    "        return self.gen_rule(random.choice(rules), depth+1, max_depth)\n",
    "\n",
    "    def gen_rule(self, rule, depth, max_depth):\n",
    "        return ''.join(self.gen_key(token, depth, max_depth) for token in rule)\n",
    "\n",
    "    def fuzz(self, key='<start>', max_depth=10):\n",
    "        return self.gen_key(key=key, depth=0, max_depth=max_depth)\n",
    "\n",
    "    def __init__(self, grammar):\n",
    "        super().__init__(grammar)\n",
    "        self.key_cost = {}\n",
    "        self.cost = self.compute_cost(grammar)\n",
    "\n",
    "    def compute_cost(self, grammar):\n",
    "        cost = {}\n",
    "        for k in grammar:\n",
    "            cost[k] = {}\n",
    "            for rule in grammar[k]:\n",
    "                cost[k][str(rule)] = self.expansion_cost(grammar, rule, set())\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_fuzzer = LimitFuzzer(EXPR_GRAMMAR)\n",
    "expr_fuzzer.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to define abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark the abstract nodes\n",
    "\n",
    "Give a list of paths that were verified as abstract, we go through each, and mark them abstract in the same tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_verified_abstract_path(tree, path):\n",
    "    name, children = get_child(tree, path)\n",
    "    new_tree = replace_path(tree, path, (name, children, {'abstract': True}))\n",
    "    return new_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mark_verified_abstract_path(te, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_verified_abstract(tree, verified_paths):\n",
    "    for path in verified_paths:\n",
    "        tree = mark_verified_abstract_path(tree, path)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "at = mark_verified_abstract(te, ([0,0], [0,2])); at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method to mark everything else as concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_concrete(tree):\n",
    "    name, children, *abstract_a = tree\n",
    "    abstract = {'abstract': False} if not abstract_a else abstract_a[0]\n",
    "    return (name, [mark_concrete(c) for c in children], abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = mark_concrete(at); t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to display the abstracted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def till_abstract(node):\n",
    "    name, children, *rest = node\n",
    "    if rest[-1]['abstract']:\n",
    "        return (name + '*', [])\n",
    "    return (name, [till_abstract(c) for c in children], *rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(till_abstract(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a method that given a list of paths, will replace each of these nodes with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_arr_with_random_values(npath_arr, grammar, tree):\n",
    "    if not npath_arr: return tree\n",
    "    npath, *npath_arr = npath_arr\n",
    "    node = get_child(tree, npath)\n",
    "    gf = LimitFuzzer(grammar)\n",
    "    e = gf.fuzz(key=node[0])\n",
    "    ntree = replace_path(tree, npath, [node[0], [(e, [])]])\n",
    "    assert ntree is not None\n",
    "    return replace_arr_with_random_values(npath_arr, grammar, ntree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = replace_arr_with_random_values([[0, 0], [0, 2]], EXPR_GRAMMAR, t); rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(rt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper for the `replace_arr_with_random_values()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(dtree, grammar, paths):\n",
    "    res = replace_arr_with_random_values([p[0] for p in paths], grammar, dtree)\n",
    "    return tree_to_string(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if we can generalize a node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define `can_generalize()`. This function takes a current path, the derivation tree, the grammar, predicate and a list of unverified paths the maximum number of checks (`max_checks`), and produces `max_checks` number of inputs where every node in unverified and the current node in `tval` is replaced with random values. This is then checked to see if the resulting input reproduces the failure. This way, we can verify that `tval` can be generalized even when other abstract nodes are replaced with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHECKS=100\n",
    "MAX_LIMIT=1000\n",
    "FIND_COUNTER_EXAMPLE=True\n",
    "MIN_EXAMPLES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_generalize(tval, dtree, grammar, predicate, unverified, max_checks):\n",
    "    checks = 0\n",
    "    limit = 0\n",
    "    abstract = True\n",
    "    rstr = None\n",
    "    checks = set()\n",
    "    while len(checks) < max_checks:\n",
    "        limit += 1\n",
    "        if limit >= MAX_LIMIT:\n",
    "            # giveup.\n",
    "            if FIND_COUNTER_EXAMPLE:\n",
    "                if len(checks) > MIN_EXAMPLES:\n",
    "                    abstract = True\n",
    "                else:\n",
    "                    abstract = False\n",
    "            else:\n",
    "                abstract = False\n",
    "            path, status = tval\n",
    "            node = get_child(dtree, path)\n",
    "            print('warn: giving up', node[0], 'after', MAX_LIMIT,\n",
    "                    'and no counterexample found.'\n",
    "                    'with', len(checks),\n",
    "                    'valid values abstract:', abstract)\n",
    "            break\n",
    "        rstr = generate(dtree, grammar, [tval] + unverified)\n",
    "        pres = predicate(rstr)\n",
    "        if pres == PRes.failed:\n",
    "            abstract = False\n",
    "            break\n",
    "        else:\n",
    "            if pres == PRes.success:\n",
    "                checks.add(rstr)\n",
    "            else:\n",
    "                continue\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_expr = [0]\n",
    "display_tree(get_child(er, first_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_expr = [0,0,0,1]\n",
    "display_tree(get_child(er, second_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_expr = [0,0,0,1,0,0,1]\n",
    "display_tree(get_child(er, third_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class St(Enum):\n",
    "    unchecked = 1\n",
    "    unverified = -1\n",
    "    verified = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_generalize((second_expr, St.unchecked), er, EXPR_GRAMMAR, my_predicate, [], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_generalize((third_expr, St.unchecked), er, EXPR_GRAMMAR, my_predicate, [], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `abstraction()` uses `can_generalize()` to check whether each node can be abstracted. The main job of `abstraction()` is to recurse into the child nodes when a previously abstract node is marked as concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_IS_SPECIAL=True\n",
    "SKIP_IS_CONCRETE = True\n",
    "LOG=True\n",
    "LOOK_DEEPER_IN_ISOLATED=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstraction(tval, dtree, grammar, predicate, unverified, max_checks):\n",
    "    global check_counter, KEY\n",
    "    path, status = tval\n",
    "    node = get_child(dtree, path)\n",
    "    KEY = \"%s:(%s) %s\" % (node[0], tree_to_string(node), status)\n",
    "    if LOG:\n",
    "        print(check_counter, 'check:', node[0], status)\n",
    "        check_counter += 1\n",
    "    key, children, *rest = node\n",
    "    if not children: return []\n",
    "    if not is_nt(key): return []\n",
    "\n",
    "    if key == '<_SKIP>' and SKIP_IS_SPECIAL:\n",
    "        if SKIP_IS_CONCRETE: return []\n",
    "        if status == St.unchecked:\n",
    "            print('abstract: unverified', node[0])\n",
    "            return [(path, St.unverified)]\n",
    "        else:\n",
    "            print('abstract: verified', node[0])\n",
    "            return [(path, St.verified)]\n",
    "\n",
    "    abstract = can_generalize(tval, dtree, grammar, predicate, unverified, max_checks)\n",
    "    if abstract:\n",
    "        if status == St.unchecked:\n",
    "            print('abstract: unverified', node[0])\n",
    "            return [(path, St.unverified)]\n",
    "        else:\n",
    "            print('abstract: verified', node[0])\n",
    "            return [(path, St.verified)]\n",
    "    else:\n",
    "        if status == St.unverified:\n",
    "            print('NOT ABSTRACT:', KEY)\n",
    "\n",
    "        if is_token(key): return []\n",
    "        paths = []\n",
    "        # what should we do when an unverified node is found not abstract?\n",
    "        # do we look at the child nodes? It can be costly, because now we\n",
    "        # are also dealing with random values in other nodes marked general.\n",
    "        if status == St.unchecked or LOOK_DEEPER_IN_ISOLATED:\n",
    "            for i,child in enumerate(children):\n",
    "                if not is_nt(child[0]): continue\n",
    "                tval = (path + [i], St.unchecked)\n",
    "                p = abstraction(tval, dtree, grammar, predicate, unverified, max_checks)\n",
    "                paths.extend(p)\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = abstraction((first_expr, St.unchecked), er, EXPR_GRAMMAR, my_predicate, [], 100); ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(get_child(er, ab[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To isolate independent causes,  We first collect all nodes that can be independently generalized, and then check whether they are still generalizable even when other abstract marked nodes are replaced with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation(tree, grammar, predicate, max_checks):\n",
    "    def _path(v): return ','.join([str(i) for i in v[0]])\n",
    "    def is_child(a, b):\n",
    "        return len(a[0]) > len(b[0]) and _path(a).startswith(_path(b)) # a is child of b\n",
    "\n",
    "    global gen_counter\n",
    "    unverified = [([], St.unchecked)]\n",
    "    verified = []\n",
    "    original = []\n",
    "    while unverified:\n",
    "        v = unverified.pop(0)\n",
    "        if LOG:\n",
    "            node = get_child(tree, v[0])\n",
    "            print(gen_counter, 'isolation:', node[0], v[1])\n",
    "            gen_counter += 1\n",
    "        _original = list(original)\n",
    "        original.append(v)\n",
    "        # problem here is, some of the original may be parent of some of the unverified\n",
    "        # so, we want to avoid those unverified if they are already part of original\n",
    " \n",
    "        # so we next remove any from original that is parent of current v\n",
    "        _original = [o for o in _original if not is_child(v, o)]\n",
    "\n",
    "        # remove from _original that is parent of any of itself.\n",
    "        _original = [x for x in _original if not [o for o in _original if is_child(x, o)]]\n",
    "\n",
    "        # next we remove from unverified any that is child of any in _original\n",
    "        _unverified = list(unverified)\n",
    "        _unverified = [u for u in _unverified if not [o for o in _original if is_child(u, o)]]\n",
    "        print('O paths:')\n",
    "        for p in (_original):\n",
    "            node = get_child(tree, p[0])\n",
    "            print(\"o>\\t\", p, \"%s<%s>\" % (node[0], repr(tree_to_string(node))))\n",
    "        print()\n",
    "        print('U paths:')\n",
    "        for p in (_unverified):\n",
    "            node = get_child(tree, p[0])\n",
    "            print(\"u>\\t\", p, \"%s<%s>\" % (node[0], repr(tree_to_string(node))))\n",
    "        print()\n",
    "        \n",
    "        newpaths = abstraction(v, tree, grammar, predicate, sorted(_unverified + _original), max_checks)\n",
    "        print('current paths:')\n",
    "        for p in newpaths:\n",
    "            node = get_child(tree, p[0])\n",
    "            print(\">\\t\", p, \"%s<%s>\" % (node[0], repr(tree_to_string(node))))\n",
    "        print()\n",
    "\n",
    "        for p in newpaths:\n",
    "            if p[1] == St.verified:\n",
    "                verified.append(p)\n",
    "            elif p[1] == St.unverified:\n",
    "                unverified.append(p)\n",
    "            else:\n",
    "                assert false\n",
    "    print('abstract paths:', len(verified))\n",
    "    new_tree = mark_verified_abstract(tree, [p[0] for p in verified])\n",
    "    # now change everything else to False\n",
    "    return mark_concrete(new_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = replace_path(parsed_expr, [0, 0], get_child(er, [0, 0])); display_tree(nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_string(nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ist = isolation(nt, EXPR_GRAMMAR, my_predicate, 100); ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(till_abstract(ist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In finding similar nodes, we have to give first preference\n",
    "to the paths with maximum *amount* of common elements that can\n",
    "be identified on a string in terms of character count. No\n",
    "similarity analysis should be allowed on the nodes where a\n",
    "previous analysis detected similarity\n",
    "\n",
    "The essential idea here is to first find _concrete_ nodes, and\n",
    "check their string representation one at a time with other concrete\n",
    "nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_g(abstract_a):\n",
    "    if not abstract_a:\n",
    "        return True\n",
    "    else:\n",
    "        return abstract_a[0]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_concrete_paths_to_nt(gtree, path=None):\n",
    "    if path is None: path = []\n",
    "    name, children, *_general = gtree\n",
    "    general = e_g(_general)\n",
    "\n",
    "    # we dont care about general non terminals\n",
    "    if general and is_nt(name): return []\n",
    "    # we dont care about terminals either\n",
    "    if not is_nt(name): return []\n",
    "    if name == '<_SKIP>': return []\n",
    "\n",
    "    my_paths = [path]\n",
    "    # for tokens we do not care about things below\n",
    "    if is_token(name): return my_paths\n",
    "\n",
    "    for i, c in enumerate(children):\n",
    "        ps = identify_concrete_paths_to_nt(c, path + [i])\n",
    "        my_paths.extend(ps)\n",
    "    return my_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nts = identify_concrete_paths_to_nt(ist); nts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nt in nts:\n",
    "    print(nt, \"          \", tree_to_string(get_child(ist, nt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to find out of these nodes, which are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_nodes(gtree, cpaths, log=False):\n",
    "    strings = {}\n",
    "    for path in cpaths:\n",
    "        n = get_child(gtree, path)\n",
    "        s = tree_to_string(n)\n",
    "        if not len(s): continue\n",
    "        key = (n[0], s)\n",
    "        strings.setdefault(key, []).append(path)\n",
    "    if log:\n",
    "        for k in strings:\n",
    "            print(k)\n",
    "            for q in strings[k]:\n",
    "                print(\"  \", q)\n",
    "    return {s:strings[s] for s in strings if len(strings[s]) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_nodes(ist, nts, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we verify that two keys are abstractable but context sensitive? generate random values for them, and replace it in all instances, and verify that the faults can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_these_similar(tkey, paths, grammar, gtree, predicate, max_checks=100):\n",
    "    name, string = tkey\n",
    "    if len(paths) < 2: return False\n",
    "    gf = LimitFuzzer(grammar)\n",
    "    nchecks = 0\n",
    "    seen = set()\n",
    "    global KEY\n",
    "    KEY = 'similar: ' + name\n",
    "    for i in range(max_checks):\n",
    "        v = gf.fuzz(key=name)\n",
    "        if v in seen: continue\n",
    "        seen.add(v)\n",
    "        # now replace it in all paths\n",
    "        ctree = gtree\n",
    "        for p in paths:\n",
    "            ctree = replace_path(ctree, p, (name, [(v, [])]))\n",
    "        res = tree_to_string(ctree)\n",
    "        pr = predicate(res)\n",
    "        if pr == PRes.failed:\n",
    "            print(repr(v), repr(res))\n",
    "            return False\n",
    "        elif pr == PRes.success:\n",
    "            continue\n",
    "        elif pr == PRes.invalid  or pr == PRes.timeout:\n",
    "            nchecks += 1\n",
    "\n",
    "    if len(seen) <= 1: # a single\n",
    "        return False\n",
    "\n",
    "    # there has been at least one successful replacement\n",
    "    return nchecks < max_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `markup_paths` simply rename the context sensitive nodes with a `$` prefix. Since there can be multiple context sensitive nodes with the same nonterminal, but separate usages, we also append a number to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsym = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markup_paths(tkey, paths, gtree):\n",
    "    global nsym\n",
    "    nsym += 1\n",
    "    name, string = tkey\n",
    "    newname = '<$%s_%d>' % (name[1:-1], nsym)\n",
    "    for p in paths:\n",
    "        cname, children, gen = get_child(gtree, p)\n",
    "        assert name == cname\n",
    "        # now it is generalizable!\n",
    "        gtree = replace_path(gtree, p, (newname, children, {'sensitive': True, 'abstract': True})) \n",
    "    return gtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_similarities(grammar, predicate, generalized_tree, max_checks=100):\n",
    "    cpaths = identify_concrete_paths_to_nt(generalized_tree)\n",
    "    similar_nodes = find_similar_nodes(generalized_tree, cpaths)\n",
    "\n",
    "    for key in similar_nodes:\n",
    "        res = are_these_similar(key, similar_nodes[key], grammar, generalized_tree, predicate, max_checks)\n",
    "        if res and len(similar_nodes[key]) > 1:\n",
    "            generalized_tree = markup_paths(key, similar_nodes[key], generalized_tree)\n",
    "        print('Similar?', key, res)\n",
    "    # from_paths_identify_similar\n",
    "    # for each similar verify if they can be fuzzed together.\n",
    "    return generalized_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to define the string representation of the tree. This is done by `general_str()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_str(tree):\n",
    "    name, children, *general_ = tree\n",
    "    if not is_nt(name): return name\n",
    "    v = tree_to_string(tree)\n",
    "    if not v.strip(): return v\n",
    "    general = e_g(general_)\n",
    "    if general:\n",
    "        if is_nt(name):\n",
    "            if name == '<>': return v\n",
    "            return name\n",
    "        else:\n",
    "            assert not children\n",
    "            return name\n",
    "    res = []\n",
    "    for c in children:\n",
    "        x = general_str(c)\n",
    "        res.append(x)\n",
    "    return ''.join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstraction(grammar_, my_input, predicate, max_checks=100):\n",
    "    start = grammar_.get('[start]', '<start>')\n",
    "    grammar = grammar_['[grammar]']\n",
    "    assert start in grammar\n",
    "    assert predicate(my_input) == PRes.success\n",
    "    d_tree, *_ = Parser(grammar, start_symbol=start, canonical=True).parse(my_input)\n",
    "    min_tree = reduction(d_tree, grammar, predicate)\n",
    "    min_s = tree_to_string(min_tree)\n",
    "\n",
    "    dd_tree_ =  isolation(min_tree, grammar, predicate, max_checks)\n",
    "    dd_tree = identify_similarities(grammar, predicate, dd_tree_, max_checks)\n",
    "    s = general_str(dd_tree)\n",
    "    return min_s, s, dd_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_s, s, dd_tree = get_abstraction(EXPR_G, my_input, my_predicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted minimal tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstract input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert general_str(dd_tree) == '((<expr>))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(till_abstract(dd_tree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_covarying_paths(tree, path):\n",
    "    name, children, *general_ = tree\n",
    "    general = e_g(general_)\n",
    "    v = tree_to_string(tree).strip()\n",
    "    if name.startswith('<$'):\n",
    "        v = name[2:-1]\n",
    "        i = v.rindex('_')\n",
    "        oname = \"<%s>\" % v[:i]\n",
    "        if oname:\n",
    "            return [(oname, name, path)]\n",
    "        else:\n",
    "            return []\n",
    "    if not is_nt(name): return []\n",
    "    if is_token(name): return []\n",
    "    if general: return []\n",
    "    paths = []\n",
    "    for i,c in enumerate(children):\n",
    "        p = find_covarying_paths(c, path + [i])\n",
    "        paths.extend(p)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_abstract_paths(tree, path):\n",
    "    name, children, *general_ = tree\n",
    "    general = e_g(general_)\n",
    "    if not is_nt(name): return []\n",
    "    if general: return [path]\n",
    "    #if A.is_token(name): return []\n",
    "    paths = []\n",
    "    for i,c in enumerate(children):\n",
    "        p = find_abstract_paths(c, path + [i])\n",
    "        paths.extend(p)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzz_tree(mgrammar, tree):\n",
    "    start = mgrammar['[start]']\n",
    "    grammar = mgrammar['[grammar]']\n",
    "    paths = find_abstract_paths(tree, [])\n",
    "    cpaths = find_covarying_paths(tree, [])\n",
    "    if len(paths + cpaths) == 0: return 'No abstract paths'\n",
    "    print('# abstract paths', len(paths + cpaths))\n",
    "    gf = LimitFuzzer(grammar)\n",
    "    results = []\n",
    "    for i in range(MAX_LIMIT):\n",
    "        abs_path_ = random.choice(paths + cpaths)\n",
    "        if isinstance(abs_path_, tuple):\n",
    "            name, fname, abs_path = abs_path_\n",
    "            #node = A.get_child(tree, abs_path)\n",
    "            #_, children, *rest = node\n",
    "            all_paths = [p for n,f,p in cpaths if f == fname]\n",
    "            e = gf.fuzz(key=name)\n",
    "            ntree = tree\n",
    "            for p in all_paths:\n",
    "                ntree = replace_path(ntree, p, (name, [(e, [])]))\n",
    "            results.append(tree_to_string(ntree))\n",
    "        else:\n",
    "            abs_path = abs_path_\n",
    "            node = get_child(tree, abs_path)\n",
    "            name, children, *rest = node\n",
    "            e = gf.fuzz(key=name)\n",
    "            res = replace_path(tree, abs_path, (name, [(e, [])]))\n",
    "            results.append(tree_to_string(res))\n",
    "    return list(set(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz_tree(EXPR_G, dd_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, total = 0, 0\n",
    "for i,s in enumerate(fuzz_tree(EXPR_G, dd_tree)):\n",
    "    v = my_predicate(s)\n",
    "    total += 1\n",
    "    if v:\n",
    "        count+=1\n",
    "count, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now provide a fully worked out example using Lua.\n",
    "\n",
    "Importing allprerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a separate grammar file which was converted from the [ANTLR Lua grammar](https://github.com/antlr/grammars-v4/blob/master/lua/Lua.g4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR_FILE='../lang/lua/grammar/lua.fbjson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have an external compiler, we also have to define how to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class O:\n",
    "    def __init__(self, **keys): self.__dict__.update(keys)\n",
    "    def __repr__(self): return str(self.__dict__)\n",
    "\n",
    "def do(command, env=None, shell=False, log=False, **args):\n",
    "    result = subprocess.Popen(command,\n",
    "        stdout = subprocess.PIPE,\n",
    "        stderr = subprocess.STDOUT,\n",
    "    )\n",
    "    try:\n",
    "        stdout, stderr = result.communicate(timeout=TIMEOUT)\n",
    "        result.kill()\n",
    "        stderr = '' if stderr is None else stderr.decode('utf-8', 'ignore')\n",
    "        stdout = '' if stdout is None else stdout.decode('utf-8', 'ignore')\n",
    "        return O(returncode=result.returncode, stdout=stdout, stderr=stderr)\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        try:\n",
    "            result.kill()\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        return O(returncode=255, stdout='TIMEOUT', stderr='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper to write the input string to a file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def I_do(prefix, cmd, src, as_string=False):\n",
    "    o = None\n",
    "    if as_string:\n",
    "        o = do(cmd.split(' ') + [src])\n",
    "    else:\n",
    "        with tempfile.NamedTemporaryFile(prefix=prefix) as tmp:\n",
    "            tname = tmp.name\n",
    "            tmp.write(src.encode('UTF-8'))\n",
    "            tmp.flush()\n",
    "            o = do(cmd.split(' ') + [tname])\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The predicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compiler is [Lua 5.3.5](https://www.lua.org/ftp/lua-5.3.5.tar.gz) compiled and linked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILER='../lang/lua/compilers/lua'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bug corresponds to the 4th bug [here](https://www.lua.org/bugs.html#5.3.5-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUG='../lang/lua/bugs/4.lua'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predicate(src):\n",
    "    o = I_do('lua', '%s --' % COMPILER, src)\n",
    "    if o.returncode == 0: return PRes.failed\n",
    "    if o.returncode == -11: return PRes.success\n",
    "    out = o.stdout\n",
    "    if 'Segmentation fault (core dumped)' in out:\n",
    "        return PRes.success\n",
    "    elif 'stack traceback' in out:\n",
    "        return PRes.invalid\n",
    "    elif 'TIMEOUT' in out:\n",
    "        return PRes.timeout\n",
    "    return PRes.failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other helpers to load the grammar and the bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grammar(grammar_fn, bug_fn, pred):\n",
    "    meta, tree = load_parsed_bug(bug_fn, grammar_fn)\n",
    "    name = os.path.basename(bug_fn)\n",
    "\n",
    "    return meta, tree, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bug(bug_fn, grammar_meta):\n",
    "    with open(bug_fn) as f: bug_src = f.read()\n",
    "    start = grammar_meta['[start]']\n",
    "    grammar = grammar_meta['[grammar]']\n",
    "    parser = Parser(grammar, start_symbol=start, canonical=True) # log=True)\n",
    "    forest = parser.parse(bug_src.strip())\n",
    "    tree = list(forest)[0]\n",
    "    return grammar_meta, coalesce(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parsed_bug(bug_fn, grammar_fn):\n",
    "    with open(grammar_fn) as f:\n",
    "        grammar_meta = json.loads(f.read())\n",
    "    return load_bug(bug_fn, grammar_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a coalesce to manage the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce(tree):\n",
    "    name, children, *rest = tree\n",
    "    if not is_nt(name):\n",
    "        return (name, children, *rest)\n",
    "    elif is_token(name):\n",
    "        v = tree_to_string(tree)\n",
    "        return (name, [(v, [])], *rest)\n",
    "    else:\n",
    "        return (name, [coalesce(n) for n in children], *rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta, tree, name = load_grammar(GRAMMAR_FILE, BUG, my_predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_string(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_predicate(tree_to_string(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(meta.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tree = reduction(tree, meta['[grammar]'], my_predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_string(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_s, abs_s, a_mintree = get_abstraction(meta,tree_to_string(tree), my_predicate, MAX_CHECKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zoom(display_tree(till_abstract(a_mintree)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, total = 0, 0\n",
    "for i,s in enumerate(fuzz_tree(meta, a_mintree)):\n",
    "    v = my_predicate(s)\n",
    "    total += 1\n",
    "    if v:\n",
    "        count+=1\n",
    "count, total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
